# README

# Android Mischief Dataset

## Introduction

The Android Mischief Dataset is a valuable resource for researchers interested in studying network traffic from mobile phones infected with Android Remote Administration Tools (RATs). This dataset was created by the Stratosphere Laboratory at the Czech Technical University in Prague, with the primary aim of aiding in the development of new detection methods for Android RATs.

## Data Description

The dataset comprises network traffic data and associated logs and screenshots for eight different Android RATs. Each RAT in the dataset is represented by the following files:

1. **Log**: This file provides a detailed time log of all actions performed by the respective RAT. It offers insights into the activities and behavior of the malware.
   
2. **Pcap**: A network traffic capture (in PCAP format) that records the RAT's activities and communications during the infection process. Analyzing the PCAP files can shed light on the network behavior of the Android RATs.

3. **Screenshots**: These are visual records of the mobile device's screen and the controller interface during the experiment. Screenshots can be particularly useful for understanding the user interface and interactions of the RATs.

4. **Zeek Logs**: Zeek is a powerful network traffic analysis tool. This dataset includes logs generated by Zeek, providing an additional layer of information for researchers to analyze the network traffic.

## Data Format

The Android Mischief Dataset contains data in multiple formats, including:

- **Pcap files**: Network traffic captures in PCAP format.
- **Log files**: Text-based logs.
- **Screenshots**: Images in PNG format.
- **Zeek logs**: Logs generated by Zeek in text format.

## Data Access

You can access the Android Mischief Dataset for free by visiting the Stratosphere Laboratory website at the following URL:

[Android Mischief Dataset](https://www.stratosphereips.org/android-mischief-dataset)

Please refer to the website for detailed information about downloading and using the dataset.

## Acknowledgment

We would like to express our gratitude to the Stratosphere Laboratory at the Czech Technical University in Prague for making this dataset available for research and contributing to the study of Android RATs.

## Data Visualization with Python

### Required Libraries and Versions

To run the code successfully, ensure you have the following Python libraries installed with the specified versions:

- **NumPy**: Version 1.0.0 or higher
- **Pandas**: Version 1.0.0 or higher
- **Matplotlib**: Version 3.0.0 or higher
- **Seaborn**: Version 0.9.0 or higher

These libraries are essential for data manipulation and visualization in this project. You can install them using pip:

```bash
pip install numpy pandas matplotlib seaborn
```

### Warning Suppression

Within the code, you'll notice the following line used to suppress warnings:

```python
import warnings
warnings.filterwarnings('ignore')
```

This is included to prevent warning messages generated by the libraries from being displayed. It can be useful for maintaining a cleaner output in your Jupyter Notebook or console. However, use this with caution, as it may hide potentially valuable information that can assist in debugging your code.

For more information on these libraries and working with warnings in Python, you can refer to their official documentation:

- [NumPy Documentation](https://numpy.org/doc/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)
- [Seaborn Documentation](https://seaborn.pydata.org/)
- [Python Warnings](https://docs.python.org/3/library/warnings.html)

## Data Preparation with Python

### Data Loading and Exploration

We are preparing data for analysis. We use the following libraries:

- **pandas**: Version 1.0.0 or higher
- **sklearn**: Version 0.22.0 or higher

These libraries are essential for data loading and splitting. You can install them using pip:

```bash
pip install pandas scikit-learn
```

## Data Processing and Concatenation

We perform data processing and concatenation of multiple datasets. To run this code, ensure you have the following Python library installed:

- **pandas**: Version 1.0.0 or higher

You can install pandas using pip:

```bash
pip install pandas
```

The code reads multiple CSV files named `RAT01.csv`, `RAT02.csv`, and so on, and loads them into individual DataFrames. A 'Target' column is added to each DataFrame, containing the corresponding file name without the extension.

Additionally, a 'Malicious' column is added to each DataFrame based on specific conditions. Depending on the file number (i from 1 to 8), the 'Malicious' column is set to 1 if certain conditions related to 'Source' and 'Destination' columns are met; otherwise, it's set to 0.

Finally, all the DataFrames are concatenated into a single DataFrame `df_merge` using `pd.concat()`, resulting in a unified dataset for further analysis.


---

# IP and Port Extraction from DataFrame

This code snippet is designed to extract source IP and destination IP information from a DataFrame using regular expressions. The code focuses on parsing information stored in the 'Info' column of the DataFrame and extracts IP addresses and port numbers.

## Prerequisites

- Python environment with the `re` module.
- A DataFrame named `df_merge` containing the 'Info' column with the desired data.

## Usage

1. Import the `re` module:

   ```python
   import re
   ```

2. Initialize two empty lists, `Sourceport_ips` and `Destination_ips`, to store the extracted IP and port information.

3. Iterate through the 'Info' column of the DataFrame:

   ```python
   for i in range(len(df_merge['Info'])):
   ```

4. Extract the relevant information using a regular expression:

   ```python
   text = str(df_merge['Info'][i])
   match = re.search(r'(\d+\s{2}\>\s{2}\d+)', text)
   ```

5. If a match is found, process and append the data to the respective lists:

   ```python
   if match:
       # Extract the IP and port information
       result = match.group(1)
       result = result.split('  ')
       if '>' in result:
           result.remove('>')
       t = int(result[0])
       Sourceport_ips.append(t)
       t1 = int(result[1])
       Destination_ips.append(t1)
   ```

6. If no match is found, add 0 to both lists as a placeholder:

   ```python
   else:
       Sourceport_ips.append(0)
       Destination_ips.append(0)
   ```

7. `Sourceport_ips` and `Destination_ips` now contain the extracted IP and port information from the 'Info' column.

## Example

Here's an example of the extracted data:

```python
Sourceport_ips = [12345, 0, 67890, ...]
Destination_ips = [54321, 0, 98765, ...]
```

## Notes

- This code assumes that the 'Info' column contains data in the specified format for successful extraction.
- Make sure to adapt the code to your specific DataFrame and data format as needed.

---

# Correlation Heatmap with Seaborn and Matplotlib

## Usage

1. Import the required libraries at the beginning of your Python script:

```python
import seaborn as sns
import matplotlib.pyplot as plt
```

2. Select the columns of interest from your DataFrame. Ensure that 'df_merge' contains the necessary columns ('Time', 'Length', 'Malicious', and 'Above_1024').

```python
selected_columns = df_merge[['Time', 'Length', 'Malicious', 'Above_1024']]
```

3. Calculate the correlation matrix for the selected columns:

```python
correlation_matrix = selected_columns.corr()
```

4. Create a figure with the desired size using Matplotlib:

```python
plt.figure(figsize=(8, 6))
```

5. Create the correlation heatmap using Seaborn. Customize the heatmap appearance and annotation options to your preference:

```python
sns.heatmap(correlation_matrix, cmap='RdBu', center=0, vmin=-1, vmax=1, annot=True, fmt='.1f', linewidths=2)
```

6. Show the heatmap using Matplotlib:

```python
plt.show()
```
# Feature Engineering for Top 10 Malicious Sources, Destinations, and Protocols

In addition to creating a correlation heatmap, this project involves feature engineering to identify and label the top 10 malicious sources, destinations, and protocols in the dataset. This information can be valuable for further analysis and model building.

### Top 10 Malicious Sources

The code identifies the top 10 malicious sources in the dataset, and for each of them, it creates a binary feature in the DataFrame 'X' to indicate whether a particular row corresponds to that source. Here's how it's done:

```python
top_10_malicious_sources = df_merge[df_merge['Malicious'] == 1]['Source'].value_counts().head(10).index.tolist()

for label in top_10_malicious_sources:
    X[label] = np.where(X['Source'] == label, 1, 0)
```

You can see the resulting DataFrame with the top 10 malicious sources features alongside the 'Source' column.

### Top 10 Malicious Destinations

Similar to the sources, the code also identifies the top 10 malicious destinations in the dataset and creates binary features for them in 'X':

```python
top_10_malicious_sources_Destinations = df_merge[df_merge['Malicious'] == 1]['Destination'].value_counts().head(10).index.tolist()

for label in top_10_malicious_sources_Destinations:
    X[label] = np where(X['Source'] == label, 1, 0)
```

Again, you can observe the DataFrame with the top 10 malicious destinations features alongside the 'Source' column.

### Top 10 Source and Destination Counts

Additionally, the code identifies the top 10 sources and destinations by count and creates binary features for them. This can help in analyzing the most frequent sources and destinations in the dataset:

```python
top_10 = [x for x in X.Source.value_counts().sort_values(ascending=False).head(10).index]

for label in top_10:
    X[label] = np.where(X['Source'] == label, 1, 0)
```

You can see the features for the top 10 sources and destinations alongside their respective columns.

### Top 10 Protocols

Finally, the code also identifies the top 10 protocols and creates binary features for them:

```python
top_10 = [x for x in X.Protocol.value_counts().sort_values(ascending=False).head(10).index]

for label in top_10:
    X[label] = np.where(X['Protocol'] == label, 1, 0)
```

You can see the features for the top 10 protocols alongside the 'Protocol' column in the DataFrame.

This feature engineering can be helpful for building models and analyzing the impact of these top sources, destinations, and protocols on the target variable 'Malicious.'


## Preprocessing Data: Converting Feature Names to Strings, Splitting, and Standard Scaling

Before training machine learning models, it's essential to preprocess the data. This section explains how to perform preprocessing tasks such as converting feature names to strings, splitting the dataset into training and testing sets, and applying standard scaling using scikit-learn.

### Importing the Necessary Libraries

To accomplish these preprocessing tasks, we need to import relevant functions and classes from scikit-learn:

```python
from sklearn.model_selection import train_test_split as tts
from sklearn.preprocessing import StandardScaler
```

### Converting Feature Names to Strings

In some cases, it's beneficial to convert feature names to strings, ensuring compatibility with various machine learning algorithms:

```python
# Convert feature names to strings
x_train.columns = x_train.columns.astype(str)
```

### Splitting the Dataset

The code then splits the dataset into training and testing sets using the `train_test_split` function from scikit-learn:

```python
# Split the data
x_train, x_test, y_train, y_test = tts(x_train, y_train, test_size=0.3, random_state=1)
```

- `x_train` and `y_train`: These are the feature variables and target variable for training.
- `x_test` and `y_test`: These are the feature variables and target variable for testing.
- `test_size`: This parameter determines the proportion of the dataset allocated to the testing set (30% in this case).
- `random_state`: Providing a specific seed value (1 in this case) ensures reproducibility of the data split.

### Standard Scaling

Standard scaling is applied to the feature variables in the training and testing sets to ensure they are on a similar scale:

```python
# Perform standard scaling
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)
```

Standard scaling transforms the data to have a mean of 0 and a standard deviation of 1, making it suitable for many machine learning algorithms.

By following these preprocessing steps, you prepare the data for model training and evaluation, ensuring that feature names are in string format, the dataset is split for testing, and features are standardized for modeling.






## Training a Random Forest Classifier

### Importing the Necessary Library

```python
from sklearn.ensemble import RandomForestClassifier
```

### Creating a Random Forest Classifier

```python
clf = RandomForestClassifier(n_estimators=5, max_depth=5, random_state=42)
```

### Training the Classifier

```python
clf.fit(x_train, y_train)
```

### Computing Accuracy on Training Data

```python
training_accuracy = clf.score(x_train, y_train)
print(f"Accuracy on training data: {training_accuracy}")
```
The machine's accuracy on the training data is approximately 92.01%.

## Saving a Trained Model

### Importing the Necessary Library

To save a trained machine learning model, you need to import the `joblib` library. You can do this with the following import statement:

```python
import joblib
```

### Saving the Model

Once you've trained your machine learning model, you can save it to a file for later use. In your code snippet, you are using the following code to save the model:

```python
model_filename = 'Random_forest_model.pkl'
joblib.dump(clf, model_filename)
```

- `model_filename`: This variable stores the filename for the saved model. You can choose any filename and extension you prefer, but it's common to use ".pkl" to indicate a pickled file.

- `clf`: This is the trained machine learning model that you want to save.

### Usage

Once the model is saved, you can load it in the future for making predictions without having to retrain the model. You can use the following code to load the saved model:

```python
loaded_model = joblib.load(model_filename)
```

The `loaded_model` will then contain the trained model, and you can use it for predictions.

### Best Practices

- It's a good practice to save your trained models, especially if they take a long time to train, to avoid repeating the training process.

- Be sure to choose an appropriate filename and location for your saved model so that you can easily locate and use it when needed.

This section explains how to save a trained machine learning model using the `joblib` library and how to later load the saved model for prediction.

# Contributing

We welcome contributions to the Android Rat Detection. If you are interested in contributing, please contact Jithin[zithinchowdhary159898@gmail.com]

## Acknowledgments

We would like to thank the Stratosphere Laboratory at the Czech Technical University in Prague for creating the Android Mischief Dataset.

## License

The Android Mischief Dataset is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0).

## Contact

If you have any questions about the Android Mischief Dataset, please contact the Stratosphere Laboratory at [	kamifai14@gmail.com]

## References

[1] Babayeva, Kamila, et al. "Execution, Analysis and Detection of Android RATs traffic." Civilsphere Project. 2020.
ChatGPT
